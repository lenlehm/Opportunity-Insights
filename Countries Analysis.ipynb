{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Idea: <br /> \n",
    "Classify gender based on the dataset: Neighborhood Characteristics by County <br /> \n",
    "Regress income household based on dataset <br /> \n",
    "with Neural Networks and Random Forests <br /> \n",
    "Explore data with: [SandDance](https://sanddance.js.org/app/) <br /> \n",
    "[XGBOOST EXPLAIN](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) <br /> \n",
    "https://ourworldindata.org/meat-production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMIND : NO JOKES!! BE SCIENTIFIC - ADD TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short Excursion to Percentiles:** <br /> \n",
    "In statistics, percentiles are used to understand and interpret data. The nth percentile of a set of data is the value at which n percent of the data is below it. In everyday life, percentiles are used to understand values such as test scores, health indicators, and other measurements. For example, an 18-year-old male who is six and a half feet tall is in the 99th percentile for his height. This means that of all the 18-year-old males, 99 percent have a height that is equal to or less than six and a half feet. An 18-year-old male who is only five and a half feet tall, on the other hand, is in the 16th percentile for his height, meaning only 16 percent of males his age are the same height or shorter. <br /> \n",
    "\n",
    "**NOW MOVING ON TO QUINTILES: ** <br />\n",
    "Dataset is just equally splitted into 5 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREAMBLE\n",
    "\n",
    "Since the past 2 projects, I needed to code every ML algorihtm from scratch - which was great to see how the packages work behind the scene - I decided to leverage Pyhton's power to its fullest by using the already available libraries in this project. <br /> \n",
    "Hence, I will make use of Tensorflow along with Keras as well as the neat Scikit Learn packages to deal with implemented Machine Learning algorithms. <br /> \n",
    "Furthermore, since this course is also called Applied Data Analysis, I will include an excerpt of the exploratory data analysis part in order to be able to follow my thoughts. <br /> \n",
    "Due to that the first practical part of this work focuses on the Data Analysis and specifically some plots among the features to detect any kind of correlation. <br /> \n",
    "\n",
    "Putting in a nutshell, if you want to explore some new, fancy ML tools, I have to disappoint you, it is mostly about understanding the data and correlation. The classification and Regression tasks are just to satisfy this projects requirements :D. <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYS-STK4155 Project #3 - Analysis of UN data\n",
    "\n",
    "Evaluation of Project number: 3 <br />\n",
    "Name: Lennart Lehmann (ERASMUS Student - UiO Code lennarl)\n",
    "\n",
    "All the Code and data can be found in [my Github Repository](https://github.com/lenlehm/Opportunity-Insights). <br />\n",
    "\n",
    "\n",
    "## Abstract \n",
    "\n",
    "Neural Networks could outperform Logistic Regression on the Classification of credit card defaults with respect to accuracy. <br />\n",
    "However, this benefit of accuracy comes to the cost of time as well as adds more complexity to search for optimal hyperparamters for the Neural Network. <br /> \n",
    "Additionally, the accuracy could just slightly be improved compared to Logistic Regression. <br />\n",
    "Hence, the computationally more expensive and more complex models of Neural Networks are not superior for the credit card default data set. <br /> \n",
    "When applying Neural Networks to Regression for terrain data of a region in Norway, they achieved slightly inferior results - but still great results - compared to traditional Regression methods such as Ordinary Least Squares (OLS) and Ridge Regression. <br /> \n",
    "Again, Neural Networks are an overkill for this low-dimensional terrain Regression problem and should not be used for this underlying dataset due to their complexity and time consumption. <br />\n",
    "\n",
    "\n",
    "## 1.) Introduction\n",
    "\n",
    "The recently published movie [\"The Game Changers\"](https://gamechangersmovie.com) reveals some interesting facts about plant-based diets along with an athlete's performance. The movie shows several world-class athletes, such as Lewis Hamilton and Arnold Schwarzenegger, along with their dieatary habits. Furthermore, the movie backs its hypothesis that a plant-based diet is not only superior to an animal-based diet - including meat, fish, eggs and dairy - with respect to performance, but is also the healthier and environmentally friendlier version. <br /> \n",
    "This controverse movie gained a lot of attention in media with a lot of people arguing against the truth behind this movie along with its shown studies.<br /> \n",
    "The amazing book of [Hans Rosling, Factfulness](https://www.amazon.com/Factfulness-Reasons-World-Things-Better/dp/1250107814), covers the topic of supporting one's opinion with data so that it is not an opinion anymore but a mere fact. Given trustworthy data of the [United Nations (UN)](http://data.un.org) and other [public available and trusted sources](https://ourworldindata.org), such as the [World Health Organization (WHO)](https://www.who.int/data/gho/data/indicators) and the [Food and Argiculture Organization of the United Nations (FAO)](http://www.fao.org/faostat/en/#data), it is possible to retrieve a huge dataset comprising most of the 195 countries around the globe along with some indicators such as alcohol consumptions per capita, GDP per capita, meat consumption per capita and many, many more. <br /> \n",
    "The movie claimed that animal-based diets are causing impaired blood flow, inflammation and an increase in blood pressure. <br /> \n",
    "Since there are (too) many studies mentioned in the movie as well as other studies from the movie's adversaries disagreeing with the movie's hypothesis, this work will try to leverage various datasets from the above mentioned sources to find some correlations in the claims of the movie and its hypothesis. <br /> \n",
    "\n",
    "\n",
    "Another indicator that the current diet in developed countries is a mere shitshow is the unfortunate rise of risk for [cancer](https://ourworldindata.org/cancer) and [cardiovascular diseases](https://ourworldindata.org/causes-of-death), which account for a big stake of deaths across the world. <br /> \n",
    "Despite the rise of [public healthcare expenditures](https://ourworldindata.org/financing-healthcare), the risk of getting cancer or a cardiovascular disease nowadays have not dropped and these diseases may even end deadly. <br />\n",
    "Hence, there obviously is a need to scrutinize the current diet along with the healthcare effects on other diseases. <br />\n",
    "\n",
    "Moreover, the [life expectancy of Americans dropped the third year in a row](https://www.cbsnews.com/news/life-expectancy-for-american-men-drops-for-a-third-year/), which is also a clear indicator that standard American diet (SAD) or other habits in America are obviously not beneficial for their health. <br /> \n",
    "However, there are other, similiar developed countries such as Japan, Hong Kong or Spain, which still achieve rising life expectancies by an even higher \"base\" life expectancy. <br />\n",
    "Due to the medication, which is supposed to be the same around the world, there need to be other criteria affecting the life expectancy and health. Thus, these cultures have to have a different lifestyle, resulting in different life expectancies. <br />\n",
    "\n",
    "This work will focus on the health aspects from the given data and derive conclusions and correlations among several indicator variables and hopefully the importance of food, which in turn can then be used in developing countries to help these folks be more performant. <br /> \n",
    "\n",
    "\n",
    "### 1.1) Current Diets\n",
    "\n",
    "The current standard American diet (SAD) consists of over 60% processed foods, such as Bread, Pastries, Sweets, Oil, etc. Another 25% stake is animal products comprising meat, egg, fish and dairy. Hence only 13% are used for Unrefined plant-foods and whole grains. [4],[5] <br /> \n",
    "This diet does not vastly differ from the European diet - unfortunately. The Europeans also get most of their energy from processed foods and animal-based products. <br /> \n",
    "However, there is not one source suggesting one dedicated diet to be the superior among the variety of diets such as ketogenic, low-carb, protein-rich, Paleo or the countless others. Every organization makes different recommendations, but they agree in one point. An ideal diet should account way more in [plant-based products as well as whole grains](https://health.gov/dietaryguidelines/2015/resources/2015-2020_Dietary_Guidelines.pdf). [6]<br />\n",
    "Another point to mention is that none of those \"popular\" diets is relinquishing on animal-based products, each of them is suggesting some chicken, steak or fish respectively. <br /> \n",
    "\n",
    "For the sake of simplicity I will mostly focus on countries that have a similiar Human-development index (HDI), since I assume that these countries have the same access to similiar food products, i.e. in both countries, America and Norway you can buy cinnamon buns or frozen pizza without any problem, however I guess this may not be the case in Kenya or in Congo. <br /> \n",
    "Hence, restricting the analysis on countries with a similiar HDI score may make more sense. \n",
    "Unfortunately, there is no nation only consuming plant-based products, which definitely biases this analysis but still might be a good starting point to compare the most meat consuming countries to the most vegetables consuming countries.<br /> \n",
    "\n",
    "### 1.2) Death Risks\n",
    "\n",
    "Taking a moment to realize what is the most common disease in the world in terms of deaths. The below picture illustrates that pretty nicely: <br /> \n",
    "\n",
    "![Death Rates Risk](plots/death_rates.PNG 'Death Risks')\n",
    "\n",
    "*Fig. 1 Death Rate Risks gathered from the entire world over 16 years. (Source: [7](https://ourworldindata.org/obesity) )*\n",
    "\n",
    "Fig. 1 contains some uncomfortable records. Too many people are dying due to high blood pressure, obesity and smoking. Those are human-made disease. Back in the days there were no tribes or folks who smoked packs of cigarettes or were obese and definietly didn't suffer from high blood pressure. <br /> \n",
    "This graph is alarming and informative at once - there needed to be some big changes in the modern lifestyle that led to these diseases and accounting for too many deaths that could have been prevented. <br /> \n",
    "According to the *Global Burden of Disease (GBD)* study 4.7 million people died prematurely in 2017 as a result of obesity as well as 10.4 million people from high blood pressure. <br />\n",
    "13% of the entire world adults are obese nowadays and these 4.7 million people could have lived longer if they would have just changed their lifestyle. To put this huge number into context, the deaths from obesity in 2017 was close to four times the number of road accidents. [7] <br /> \n",
    "I claim that four death risks out of the top 5 could be drastically alleviated by just changing the lifestyle of these people suffering from those diseases. <br /> \n",
    "I will try to prove this hypothesis with the help of the datasets. <br />\n",
    "\n",
    "\n",
    "### 1.3) Dataset Representativity\n",
    "\n",
    "Keep in mind that this dataset comprises information about the entire country as a whole and its population living there. The studies/ experiments of the movie mostly used a restricted sample size of people across different nations and histories. <br /> \n",
    "To make this point clearer, the dataset takes an average/ stereotype french person, who drinks $x$ liters of wine a year and eats $y$ kg of meat and chesse a year. Moreover, the dataset takes the average of the specific variable, i.e. the Wine consumption per capita is calculated by dividing the sum of Wine sold in the respective country by its population size, disregarding all the masses of tourism in that respective country, which definitely accounts for some liters not being drunk from the french folks. France for example is well-known for its wine consumption - which in fact is remarkable - but considering France being the most attractive destination for travellers, France experienced [438 million tourists in 2018](https://www.thelocal.fr/20190410/peak-year-for-tourists-in-france-despite-yellow-vests-and-rail-strikes), which definitely account for some liters of wine sold and thus the wine consumption statistics can be highly biased due to tourists. <br />\n",
    "However, I still assume that this dataset comprises a representative population out of the countries' citizens. <br /> \n",
    "\n",
    "To put it in a nutshell, enjoy this work with care, since some datapoints might not be as representative as others and some datapoints might be biased due to tourists coming into the country as well as individuals living in that country, who are the complete opposite of the countries' stereotype. <br /> \n",
    "I know that there is a lot of room for counter arguments, but I try to prove the hypothesis of the movie's authors. <br />\n",
    "\n",
    "Furthermore, all the statistics with alcohol (wine, beer, spirits consumptions) was recorded for 15+ year-old citizens in that country. Thus the average will often be lower than the actual consumption in countries like the states where alcohol is only allowed to drink from 21+ years. <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Theory\n",
    "\n",
    "Since this work will make use of *Random Forests*, *XGBoost* as well as *Neural Networks*, the mathematical foundations for each of this algorithms is explained in the following section. <br />\n",
    "\n",
    "\n",
    "### 2.1) Decision Trees \n",
    "\n",
    "Decision trees are a common and still state-of-the-art used supervised machine learning method, which can even match the performance of simple Neural Networks. <br /> \n",
    "The biggest advantage is that they are very interpretable while a neural network is more like a black box, where it is often not easy to understand what exactly happend in between the (hidden) layers. <br />\n",
    "The decision tree structure makes it easy to follow its prediction and also intuitive to understand this technique. <br />\n",
    "Another neat property of Decision Trees are that they can perform both, Regression and Classification tasks. Their functions correspond to those of logistic and linear regression, where the latter is used to predict continous values (Regression) and the former focuses on classifying a specific class (Classification). <br />\n",
    "Since they are so popular and achieved remarkable results, there are many variations/ algorithms for creating these trees. However, the general concept behind those algorithms is the same for each of them. <br /> \n",
    "Most common are Classification and Regression Trees *(CART)*, *ID3* and *C4.5 algorithms*, each of them with their distinctive drawbacks and assets. <br />\n",
    "\n",
    "The main idea of decision trees is to find descriptive features containing the most information regarding the target. <br />\n",
    "The entire dataset can then be split along the values of these features resulting in an as pure as possible underlying dataset. <br />\n",
    "Hence, it is necessary to determine to most informative features, where the dataset can further be split. <br />\n",
    "This is done by checking all possible features for their so-called information gain. <br /> \n",
    "It counts to use a criterion $i(t)$ that measures how pure the class distribution at node $t$ is. It should either be maximum - if the classes are equally distributed in the node, or minimum (usually 0) - if the node is pure. <br />\n",
    "The most common impurity measures are misclassification rate, entropy and the gini index, which are defined as follows: <br />\n",
    "\n",
    "Misclassification Rate: <br /> \n",
    "\n",
    "$$\n",
    "i_E(t) = 1 - max_c p(y=c|t), \\tag{1} \n",
    "$$\n",
    "\n",
    "where $i_E$ is the misclassification rate at node $t$. <br />\n",
    "\n",
    "Entropy: <br /> \n",
    "\n",
    "$$\n",
    "i_H(t) = - \\sum_{c_i \\in C} p(y=c_i|t) log(p(y=c_i|t)), \\tag{2} \n",
    "$$\n",
    "\n",
    "where $i_H$ is the entropy at node $t$. <br />\n",
    "\n",
    "Gini index: <br /> \n",
    "\n",
    "$$\n",
    "i_G(t) = \\sum_{c_i \\in C} p(y=c_i | t) (1 - p(y=c_i |t) ) = 1 - \\sum_{c_i \\in C} (p(y=c_i|t))^2, \\tag{3} \n",
    "$$\n",
    "\n",
    "where $i_G$ is the Gini index at node $t$.[1],[2] <br />\n",
    "\n",
    "<br />\n",
    "A decision tree is typically divided into a root node, interior nodes and the final leaf nodes.\n",
    "The leaf nodes contain the predictions for the new query on the previously trained tree.\n",
    "Due to the model learning the underlying structure of the training data, it is then possible to make predictions about the target value or class respective of unseen queries. <br />\n",
    "\n",
    "\n",
    "### 2.1) Classification Trees\n",
    "\n",
    "As mentioned in the previous part, it is crucial to determine the descriptive features along with their values that split the data best with respect to their true label. <br />\n",
    "The tree is then split based on the nodes, which \"asks questions\" about the underlying dataset, i.e. \"Is the value of this feature smaller than 5\". The data is then split according to the features value, either in the node \"yes, smaller than 5\" (True) or \"no, bigger or equal to 5\" (False). <br />\n",
    "This process is done recursively, so that both of the new nodes will be asked other questions, which will\n",
    "further split the data. The splitting of each branch is performed until a stopping criteria\n",
    "is reached. <br /> \n",
    "Obviously, the Mean-Squared Error (MSE) cannot be used for a criterion to make the splits since it only measures how far apart the prediction and the true value is. The classification trees utilize the Classification Error Rate. <br />\n",
    "The Classification Error Rate is the fraction of the training observations that don't belong to the most common class. <br /> \n",
    "![Decision Tree](plots/decisionTree.PNG \"An exemplary decision tree\")\n",
    "*Fig. 2 Example of a decision tree, taken from Murphy. Tree-like structure representing a simple decision tree with different splitting criteria (categorical and numerical). A new input is then propagated through the tree and ending up in one of the leaf nodes displayed according to its feature values. (Source: [1] Chapter 16.2.1)* <br />\n",
    "\n",
    "When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. <br />\n",
    "\n",
    "\n",
    "### 2.2) Random Forests\n",
    "\n",
    "A random forest, as the name already hints, is an aggregation of a (large) group of trees, where each tree is either trained on different subsets of the data or has some other structure. <br />\n",
    "As in bagging a number of decision trees is built on bootstrapped training samples.\n",
    "Random forest has one additional stochastic element to it. In addition to bootstrapping the data sets, only a randomly selected subset of features are considered at each step of the building. This is done to counteract possible correlations of the features in the data set, as these features may not coexist in many trees. <br /> \n",
    "This will result in a so-called *ensemble* of different trees, which all describe the data differently to some degree. <br /> \n",
    "Since the *wisdom of the crowd* usually achieves outstanding results, the idea behind random forests is that the average of all those trees will yield a great performance and a low-variance result. <br />\n",
    "The size of the subset of features is a hyperparameter that can be tweaked, but a good starting point for the size of the subset of features is $m = p$, where p is the size of the full set [8]. <br />\n",
    "Once the forest is created, it can be used to classify new samples from unseen query instances. <br />\n",
    "\n",
    "Each sample is then propagated through every tree in the forest, each giving its own prediction. The\n",
    "predictions for each class aggregate and thus, the class with the highest count is\n",
    "deemed \"the winner\" according to the principles of majority voting. <br /> \n",
    "Since each tree in the forest is different from one another, obtaining the averaged result of the forest is more likely to be the truth value rather than a single decision tree. <br />\n",
    "Hence, one can expect a random forest to generally perform better than single trees. The \"bootstrapped\" nature of forests also makes them perform comparably well on sparse data. <br /> \n",
    "However, random forests are still prone to perform quite bad when subjected to skewed data sets, as the bootstraping may cause the sparsely represented classes to appear even less, resulting in a poorly representation in the forest. \n",
    "Leveraging resampling techniques, which aim to balance the uneven data sets may help the random forests to some degree. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) XGBoost\n",
    "\n",
    "[eXtreme Gradient BOOSTing (XGBoost)](https://arxiv.org/pdf/1603.02754.pdf)[3] is an extreme popular method for tabulated data and was part of the winning algorithms for several Kaggle challenges. Thus, testing this version for the UN dataset is definitely a promising approach. <br /> \n",
    "It is a supervised learning method that is based on function approximation by optimizing specific loss functions as well as applying several regularization techniques. <br /> \n",
    "\n",
    "First, letâ€™s understand the concept behind boosting. <br />\n",
    "Boosting is an ensemble method that seeks to create a strong classifier  based on 'weak' classifiers. In this context, weak and strong refer to a measure of how correlated the learners are to the actual targets. <br />\n",
    "By iteratively adding models on top of each other, the errors of the previous model are corrected by the next predictor, until the training data is accurately predicted or reproduced by the model. <br /> \n",
    "Gradient boosting also comprises an ensemble method that sequentially adds predictors and corrects previous models. \n",
    "However, instead of assigning different weights to the classifiers after every iteration, this method fits the new model to new residuals of the previous prediction and then minimizes the loss when adding the latest prediction. <br />\n",
    "In the end the model is updated using gradient descent and thus the name, gradient boosting. <br />\n",
    "The neat property of this algorithm is that it supports both regression and classification problems. <br />\n",
    "XGBoost specifically, implements this algorithm for decision tree boosting with an additional custom regularization term in the objective function. <br />\n",
    "\n",
    "Looking at the objective or loss function of XGBoost: [3] <br /> \n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} = \\sum_{i=1}^n \\ell \\left( y_i, \\hat{y_i}^{(t-1)} + f_t(x_i)\\right) + \\Omega(f_t), \\tag{4}\n",
    "$$\n",
    "\n",
    "where $y_i$ is our known target from the training data set and $\\hat{y}^{(t-1)}$ is the prediction of the last timestep. <br /> \n",
    "One can see that this objective function \"cannot be optimized using traditional optimization methods in Euclidean space\". [3] <br /> \n",
    "Therefore, it is necessary to transform this objective function into the Euclidean domain in order to optimize using the traditional techniques such as gradient descent. Thus, the approximation is done leveraging the Taylor Theorem: (Below is the second-order Taylor approximation displayed)  <br /> \n",
    "\n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x-a) + 1/2 f''(a)(x-a)^2, \\tag{5}\n",
    "$$\n",
    "\n",
    "where in our case $f(x) = \\mathcal{L}$, the loss function, while $a$ is the predicted value of the previous time step $(t-1)$ and $(x-a)$ is the new learner to add in step $t$ in order to minimize the objective. <br />\n",
    "When applying this for each iteration $t$, we can reqrite the objective as a function of the new added learner and thus can apply traditional optimization techniques. <br />\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_i^n \\left( \\ell(y_i, \\hat{y}^{(t-1)}) + g_if_t(x_i) + 1/2 h_if_t^2(x_i) \\right) + \\Omega(f_t), \\tag{6}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "g_i = \\partial_{\\hat{y}^{(t-1)}} \\ell(y_i, \\hat{y}^{(t-1)}) \\text{    and   } h_i = \\partial^2_{\\hat{y}^{(t-1)}} \\ell(y_i, \\hat{y}^{(t-1)}). \\tag{7}\n",
    "$$\n",
    "\n",
    "When removing the constant parts, it yields the following simplified objective to minimize at step $t$: <br />\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^{(t)} = \\sum_i^n \\left( g_i f_t(x_i) + 1/2 h_i f_t^2(x_i) \\right) + \\Omega(f_t). \\tag{8}\n",
    "$$\n",
    "\n",
    "Equation (8) is a sum of quadratic functions of one variable and thus can be minimized by known methods. <br /> \n",
    "Now, finding an appropriate learner that minimizes the loss function at iteration $t$. <br /> \n",
    "\n",
    "$$\n",
    "argmin_x Gx + 1/2 Hx^2 = -\\frac{G}{H}, H > 0 \\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "min_x Gx + 1/2 Hx^2 = -1/2 \\frac{G^2}{H}. \\tag{10}\n",
    "$$\n",
    "\n",
    "In order to find the next learner, the authors refer to a way to \"measure the quality of a tree structure q\" [3], and the scoring function is as follows: <br /> \n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}^{(t)}}(q) = - 1/2 \\sum_j^T \\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T, \\tag{11}\n",
    "$$\n",
    "\n",
    "where $(\\sum_{i \\in I_j} g_i)^2$ are instances mapped to leaf j. Unfortunately it also is impossible to \"enumerate all the possible tree structures q\". [3] <br />\n",
    "Hence, in practice to build a learner, one starts with the single root node that contains all training examples.\n",
    "Then iterates over all features along with their values and evaluate each possible split: <br /> \n",
    "gain = loss(parent instance) - (loss(left branch) + loss(right branch)). <br /> \n",
    "The gain for the best split has to be positive, otherwise we must stop growing the branch. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Clustering\n",
    "\n",
    "Clustering is a popular unsupervised Machine Learning algorithm to identify structure in the data. The main goal of clustering is to group similiar datapoints together and discover an underlying pattern. <br /> \n",
    "A cluster refers to a collection of datapoints aggregated together due to certain similiarities. [1], [2] <br /> \n",
    "\n",
    "\n",
    "#### KMeans\n",
    "\n",
    "KMeans is one of the most simple and popular cluster algorithms out there due to its simplicity and great interpretability. <br /> \n",
    "KMeans looks for a fixed number, $k$, of clusters in a dataset to group similiar datapoints together based on their distances to the cluster center. <br /> \n",
    "Defining a cluster centroid number,$k$, which refers to the number of centroids in the dataset. A centroid is the location representing the center of the cluster. Hence, this algorithm assumes $k$ cluster centers in the dataset. <br /> \n",
    "Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. [2]<br />\n",
    "To put it differently, the KMeans algorithm identifies $k$ number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. <br />\n",
    "As the name already hints, the \"means\" in KMeans refer to averaging of the data; that is, finding the centroid. <br />\n",
    "The algorithm starts by randomly initializing the $k$-centroids, $\\mu_k$, and iteratively optimizing the position of the cluster centers until they have converged, i.e. don't move any more or the number of maximal iterations have been achieved. <br /> \n",
    "\n",
    "After the centroids have been initialized, distances between each datapoint and the centroids, $\\mu_k$, are calculated and assigning the nearest cluster centroid to the respective datapoint. New clustercentroid positions are updated, i.e. the mean in the assigned cluster datapoints. If the centroids position change, the algorithm repeats the distance calculation to every datapoint again until it converges. <br /> \n",
    "\n",
    "Common distance metrics are the Manhattan distance: <br /> \n",
    "\n",
    "$$\n",
    "d(x_i, x_j) = \\sum_d |x_{id} - x_{jd}| ,\\tag{12}\n",
    "$$\n",
    "\n",
    "Euclidean distance: <br /> \n",
    "\n",
    "$$\n",
    "d(x_i, x_j) = \\sqrt{\\sum_d (x_{id} - x_{jd})^2 } ,\\tag{13}\n",
    "$$\n",
    "\n",
    "and the Mahalanobis distance: <br /> \n",
    "\n",
    "$$\n",
    "d(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = \\sqrt{\\sum_d (\\boldsymbol{x_i} - \\boldsymbol{x_j})^T \\Sigma^{-1} (\\boldsymbol{x_i} - \\boldsymbol{x_j}) } ,\\tag{14}\n",
    "$$\n",
    "\n",
    "where $x_i$ is one datapoint/ vector and $x_j$ is another one, or the centroid respectively. <br /> \n",
    "For the Mahalanobis distance (14), $\\Sigma$ is the covariance matrix. If the covariance matrix $\\Sigma$ is the identity matrix it equals the Euclidean distance. The covariance matrix is used to leverage the information in the shapes of the sets, i.e. whether it is spread out over large range or a small range. <br /> \n",
    "\n",
    "The KMeans objective, also known as distortion measure, is defined as: <br /> \n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{X, Z, \\mu}) = \\sum_{i=1}^N \\sum_{j=1}^K z_{ik} || \\boldsymbol{x_i} - \\boldsymbol{\\mu}_k ||^2_2,\\tag{15}\n",
    "$$\n",
    "\n",
    "where the goal is to find the best centroids $\\mu$ and cluster assignments $\\boldsymbol{Z}$: <br /> \n",
    "\n",
    "$$\n",
    "\\boldsymbol{Z}^* \\boldsymbol{\\mu}^* = argmin_{Z, \\mu}J(\\boldsymbol{X, Z, \\mu}).\\tag{16}\n",
    "$$\n",
    "\n",
    "Equation (16) will end up in a joint optimization problem: <br /> \n",
    "\n",
    "$$\n",
    "min_{Z, \\mu}J(\\boldsymbol{X, Z, \\mu}) = min_{Z, \\mu} \\sum_{i=1}^N \\sum_{j=1}^K z_{ik} || \\boldsymbol{x_i} - \\boldsymbol{\\mu}_k ||^2_2,\\tag{17}\n",
    "$$\n",
    "\n",
    "resulting in two subproblems $min_Z$ and $min_{\\mu}$, which can be solved via alternating optimization, which updates $\\boldsymbol{Z}$ and $\\boldsymbol{\\mu}$ in turns. A method to solve the KMeans is the Llyod's algorithm. [2] <br /> \n",
    "\n",
    "With every iteration the centroids $\\mu$ will get closer to their \"true mean\" and at some point converge. <br />\n",
    "\n",
    "One problem in this algorithm and also many other clustering algorithms (Spectral clustering, Gaussian Mixture models, etc.) is that one has to define the number of clusters $k$. This is by far not trivial, since it is crucial to the performance and success of the algorithm. The smart folks of [Stanford created a visualization](https://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html) of the KMeans algorithm and there it is easy to see that the proper number of clusters is essential for KMeans. <br /> \n",
    "There are different menthods to estimate the number of clusters such as the Elbow methods or the Bayesian Information Criterion (BIC). <br /> \n",
    "\n",
    "#### Elbow Method\n",
    "\n",
    "The sharp minds of [Stanford published a paper describing the gap statistics](http://web.stanford.edu/~hastie/Papers/gap.pdf), where it is possible to estimate the number of clusters in a dataset. [11] <br /> \n",
    "By calculating the average in-cluster sum of squared distances and plotting it against the number of clusters one can find the so-called \"elbow\" visually, which refers to the optimal cluster number. <br /> \n",
    "The average in-cluster sum is the average distance between points inside of a cluster: <br />\n",
    "\n",
    "$$\n",
    "W_k = \\sum_{r=1}^K \\frac{1}{n_r}D_r, \\tag{18}\n",
    "$$\n",
    "\n",
    "where $k$ is the number of clusters, $n_r$ is the number of points in the cluster $r$ and $D_r$ is the sum of distances between all points in that respective cluster: <br /> \n",
    "\n",
    "$$\n",
    "D_r = \\sum_{i=1}^{n_r - 1} \\sum{j=1}^{n_r} || x_i - x_j ||. \\tag{19}\n",
    "$$\n",
    "\n",
    "When plotting average in-cluster sum, $W_k$, over the number of clusters, it is possible to visually see the number of clusters, which is where the graph begins to flatten significantly. <br /> The point, where the graph starts to smooth out is the \"elbow\", which refers to the optimal cluster number $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Neural Networks\n",
    "\n",
    "Neural networks, as the name already hint, are supposed to mimic the human brain. Recent advances in Neural networks, especially Deep Neural Networks (DNN), was a breakthrough of these algorithms. These networks consist of an input layer, where the data is fed in, arbitrary many hidden layers as well as an output layer (see Fig. 1).\n",
    "![DNN Architecture](plots/deepLearning.PNG \"DNN architecture\")\n",
    "*Fig. 2 Systematic architecture of a DNN with three hidden layers (blue rectangles) and four neurons or units per hidden layer (white circles inside blue rectangle). (Source: [9](https://github.com/lenlehm/Classification-and-Regression/blob/master/E04-Deep_Learning.pdf))* <br />\n",
    "A neural net is considered *deep*, when it has multiple hidden layers, thus the depicted network is considered a *deep neural network* since it utilizes three hidden layers. <br /> \n",
    "Each of the inputs and neurons respectively have so-called *weights* $\\boldsymbol{W}$ along with biases $b$ per neuron that it counts to optimize. In Fig. 1 each of the edges you see from one neuron to another has a specific weight $w_{l,i}$, where $l \\in (0, 1, \\cdots, L)$ denotes the current layer of maximum layer size $L$ and $i \\in (0, 1, \\cdots, N)$ describes the respective neuron in that layer, where $N$ is the last Neuron in that specified layer $l$. The biases are not depicted in Fig. 1, but there is one bias value per neuron, thus resulting in $b \\in (0, 1, \\cdots, N)$ Biases per layer $l$ with $N$ neurons.<br /> \n",
    "This architecture is also called *Mulit-layered Perceptrons (MLP)* where a MLP is build from layers of connected neurons.\n",
    "The input of the network is propagated through the layers and processed by each neuron in the network. That is also the reason why these networks are called *feed forward neural networks (FFNN)*, because the information flows through the network in forward direction (from input through layers to output - see Fig. 1). <br /> \n",
    "The network itself outputs a value for a single neuron output i.e. binary classification or regression, or a vector for multi-class classification i.e. 200 classes in the ImageNet challenge (ILSRVC, see Introduction). <br /> \n",
    "As you probably could already tell, the parameters of these network explode, the deeper (more layers) and wider (more neurons per layer) we get. For instance the DNN in Fig. 1 has a 3D input $(x_1, x_2, x_3)$ and a 2D output $(y_1, y_2)$. In between it has 3 hidden layers with 4 neurons each layer, thus resulting in 64 parameters: <br />\n",
    "$ (3 \\times 4)_{\\boldsymbol{W_0}} \\cdot (4 \\times 4)_{\\boldsymbol{W_1}} \\cdot (4 \\times 4)_{\\boldsymbol{W_2}} \\cdot (4 \\times 2)_{\\boldsymbol{W_3}} + (3 \\times 4)_{\\boldsymbol{B}} = 64.$\n",
    "\n",
    "Note, that the addition at the end depicts the Bias vector, each neuron has a single bias value. Since we have 3 (layers) $\\times$ 4 (neurons in each layer), we have to add 12 parameters for the biases. <br /> \n",
    "This parameter space blows up pretty fast, especially considering complex tasks, where the input is higher dimensional such as 100-D input, which is not uncommon for real-world applications. <br />\n",
    "In order to optimize for the desired weights and biases an efficient algorithm for calculating the gradient of the entire function with repsect to the parameters is necessary. <br /> \n",
    "Here comes the *backpropagation* very handy.\n",
    "\n",
    "#### Backpropagation \n",
    "\n",
    "As the name already states, there is not only a forward propagation, but also a *backpropagation*. This can olny be applied when a forward pass was done previously, since the calculated outut is necessary to start the backward propagation. During the forward pass the network calculates the outputs of each layer with respect to the activation function $A$: <br />\n",
    "\n",
    "$$\n",
    "O_1 = A(\\boldsymbol{W_0}^T \\cdot I), \\quad \\quad where \\quad I = Input. \\tag{12}\n",
    "$$\n",
    "\n",
    "The activation function \"activates\" the neurons, which can also be rewritten with: <br /> \n",
    "\n",
    "$$\n",
    "z_j^l = \\sum_{i=1}^n w_{ij}^l\\cdot x_i + b_j^l \\tag{13} \n",
    "$$ \n",
    "\n",
    "as <br /> \n",
    "\n",
    "$$\n",
    "a_j^l = f_l(z_j^l) = f_l(\\sum_{i=1}^n w_{ij}^l \\cdot x_i + b_j^l). \\tag{14}\n",
    "$$\n",
    "\n",
    "This is done for each layer, until arriving at the output layer. After the forward pass is done, the cost function is calculated along with its derivative w.r.t. the weights and biases in the output layer $W^L$. <br /> \n",
    "Luckily, through the Chain rule of the gradients this allows us to chain the following gradients in the following manner: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial z_j^L} \\cdot \\frac{\\partial z_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot f'_L(z_j^L)a_k^{L-1}, \\tag{15}\n",
    "$$\n",
    "\n",
    "where <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial a_{j}^L} = \\frac{\\partial}{\\partial a_{j}^L} \\cdot \\left[\\frac{1}{2} \\sum_{i=1}^N (a_j^L - t_j)^2\\right] \\\\\n",
    "= a_j^L - t_j, \\tag{16}\n",
    "$$\n",
    "\n",
    "and <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^L_j}{\\partial w_{jk}^L} = \\frac{\\partial}{\\partial w_{jk}^L} \\cdot \\left[\\sum_{p=1}^N w_{jp}^L \\cdot a^{L-1}_j + b_j^L\\right] \\\\\n",
    "= a_j^{L-1}. \\tag{17}\n",
    "$$\n",
    "\n",
    "Defining everything except $a_k^{L-1}$ in (17) as $\\delta_j^L$ we end up in : <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\delta_j^L \\cdot a_k^{L-1}. \\tag{18}\n",
    "$$\n",
    "\n",
    "Guess what's up next: the lovely Chain rule ... again (applied to $\\delta_j^L$). <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\frac{\\partial f^L}{\\partial z_j^L} = \\frac{\\partial C(W^L)}{\\partial a_j^L} \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\ \n",
    "= \\frac{\\partial C(W^L)}{\\partial z_{j}^L} = \\frac{\\partial C(W^L)}{\\partial b_{j}^L} \\frac{\\partial b_j^L}{\\partial z_{j}^L} \\\\\n",
    "= \\frac{\\partial C(W^L)}{\\partial b_{j}^L}, \\tag{19}\n",
    "$$\n",
    "\n",
    "where making use of: <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_j^L}{\\partial z_{j}^L} = \\left[\\frac{\\partial z_j^l}{\\partial b_j^L} \\right]^{-1} \\\\\n",
    "= \\left[\\frac{\\partial}{\\partial b_j^L} \\sum_{i=1}^N{L-1} w_{ij}^L \\cdot a_i^{L-1} + b_j^L \\right]^{-1} \\\\\n",
    "= 1. \\tag{20}\n",
    "$$\n",
    "\n",
    "These are the derivatives of the cost function w.r.t. both weights and biases in the output layer $W^L$ and $\\boldsymbol{b}^L$. <br /> \n",
    "The following equation holds for any layer, except the output layer: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}. \\tag{21}\n",
    "$$\n",
    "\n",
    "Connecting this to the derivatives w.r.t. the next layer $l+1$: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l} = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\cdot w^{l+1}_{kj} \\cdot \\frac{\\partial f^l}{\\partial z_j^l}, \\tag{22}\n",
    "$$\n",
    "\n",
    "with <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} a_k^l + b_k^{l+1} \\right] \\\\\n",
    "= \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} f^l(z_k^l) + b_k^{l+1} \\right] \\\\\n",
    "= w_{jk}^{l+1} \\cdot f^l(z_j^l). \\tag{23}\n",
    "$$\n",
    "\n",
    "Backpropagation is usually iterating Equation (23) and computing the gradients $\\partial C / \\partial w_{ij}^l$ and $\\partial C / \\partial b_i^l$ for each layer. <br /> \n",
    "\n",
    "#### Cost Functions\n",
    "\n",
    "All the above mentioned formulas share the same cost function C. \n",
    "The cost function is really important in the learning step since it is directly correlated with the accuracy. Finding an optimal cost function is often not easy. <br /> \n",
    "The cost function is an indicator of how well the network is doing. High loss indicates that the model is far away from the true values. Likewise, a low loss means that the model can fit well on the data and thus a low loss is desirable. <br />\n",
    "Most popular and common cost functions are *Mean-Squarred Error*, which is defined as: [2] <br />\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2. \\tag{24}\n",
    "$$\n",
    "\n",
    "MSE is used in Regression tasks, since it gives a great score of how far away the predicted value from the true value is. <br />\n",
    "Beside MSE, there is also the popular Loss function called *Cross Entropy*, which is used in classification, because there is a known set of possible outcomes i.e. the classes. *Cross Entropy* is defined as: [2] <br />\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{x \\in \\mathcal{X}} p(x) log(q(x)), \\tag{25}\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are discrete probability distribution with the same $\\mathcal{X}$. <br />\n",
    "When dealing with a binary classification problem, where only two classes are present (thus $y_i \\in \\{0, 1\\}$) one could simplify (25) to: <br />\n",
    "\n",
    "$$\n",
    "CE_{binary} = - \\frac{1}{N} \\sum_{i = 1}^N y_i log(p(y_i)) + (1-y_i) log( 1 - p(y_i)). \\tag{26}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "When the gradients are known, the paramters can be updated by gradient descent according to following formula: <br /> \n",
    "\n",
    "$$\\\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\nabla C(\\theta^{(t)}), \\quad \\tau > 0 \\tag{27}\n",
    "$$\n",
    "\n",
    "for timestep $t$ and $\\tau$ being the well-known and headache provoking *learning-rate*. Too small values for $\\tau$ (< 0.000001) will take years until convergence is reached. However, too big values for $\\tau$ (> 0.01) and again no convergence is reached, since the updates are too big and the algorithm jitters around. [1]\n",
    "Since the aim is to minimize and go towards the negative gradient (descent) we have to put a \"minus\" in the update formula (27). Gradient descent is the most common optimization algorithm in machine learning, due to its performance and simplicity. A model $m(\\theta)$ is fit on a dataset $X$ with a cost function $C(X, m(\\theta))$, which evaluates the model on the underlying observations $X$. The model is fit by calculating the gradients and thus finding the optimal parameters $\\theta$ that minimize the cost function $C(X, m(\\theta)$. <br /> \n",
    "As everything in life this method also has its drawbacks. There are usually more non-convex, high-dimensional cost functions that often result in local minima instead of a global minimum. Here the inital guess (timestep $t = 0$) of $\\theta^0$ is crucial for the performance of the Gradient descent and thus sensitive to the inital guess. <br /> \n",
    "As mentioned earlier, Gradient Descent is very sensitive for the learning rate $\\tau$. There is a lot of research to find the optimal learning rate. Most of the recent published work, utilizes an adaptive learning rate, where the initial learning rate is high, but with each epoch the learning rate is reduced until it gets very small. <br /> \n",
    "Additionally the gradient is a function of $\\boldsymbol{x} = (x_1, \\ldots, x_n)$, which makes it expensive to compute numerically. <br /> \n",
    "One can alleviate the shortcomings by introducing randomness, i.e. when training in batches such as the Stochastic Gradient Descent (SGD). <br /> \n",
    "\n",
    "#### Stochastic Gradient Descent \n",
    "\n",
    "Stochastic Gradient Descent (SGD) address the drawbacks of vanilla gradient descent. The idea behind SGD is that the cost function can mostly be rewritten as a sum over datapoints: <br />\n",
    "\n",
    "$$\n",
    "C(\\theta) = \\sum_{i=1}^n c_i(\\boldsymbol{x}_i, \\theta). \\tag{28}\n",
    "$$\n",
    "\n",
    "Since the gradient is the working horse, this can also be computed as the sum over i-gradients: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_i^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{29}\n",
    "$$\n",
    "\n",
    "Randomness is added by only taking the gradient on a subset of the data, often referred to *minibatches*. Assuming $n$ datapoints and the size of minibatches $M$, there will be $\\frac{n}{M}$ minibatches. In the following of this report, the minibatches are denoted by $B_k$ where $k = 1, \\ldots, \\frac{n}{M}$. For instance one chooses $M = n$, yieldig a single datapoint in the minibatch: $B_k = \\boldsymbol{x}_k$, or $M=1$, then there is only one (Mini-)Batch $B_1$ containing all datapoints. <br /> \n",
    "Approximating the gradient by replacing the sum over all datapoints by the sum over a randomly picked minibatch in each gradient descent step: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_{i=1}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta) \\to \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{30}\n",
    "$$\n",
    "\n",
    "Accordingly an update step now looks as follows: \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta), \\tag{31}\n",
    "$$\n",
    "\n",
    "where each minibatch $B_k$ is picked randomly with equal probability from the interval $[1, \\frac{n}{M}]$. One iteration over all minibatches is known as *epoch*. Hence, it is common to choose a number of epochs instead of iterating over minibatches. <br /> \n",
    "Taking the gradient on a subset of the data introduces not only radomness, which decreases the chances to get stuck in a local minimum, but also has some computational benefits, if the minibatch size are relatively small to the number of datapoints. Common sizes of Minibatches start from [16, 32, 64, 128, 256, 512], depending on the dataset at hand.\n",
    "\n",
    "#### Adaptive Moment Estimation (ADAM)\n",
    "\n",
    "[ADAM](https://arxiv.org/abs/1412.6980)[10] is another optimization algorithm, that was introduced in 2015 on the International Conference on Learning Representations (ICLR). It was a huge achievement, by boosting the performance of the optimization problems. <br />\n",
    "SGD maintains a single learning rate $\\tau$ for all update steps and thus the learning rate does not change during training.\n",
    "ADAM instead computes adaptive learning rates for different parameters from estimates of first and second moments (*momentum term*) of the gradients. <br />\n",
    "By keeping a part of the change at the previous timestep, thus giving the optimization a momentum to accelerate the minimization in parameter space directions in which the gradient is not steep, but consistently has a small value steadily in one direction. Each minibatch changes to: [10] <br /> \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\left[\\eta \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t-1)} \\right] - \\tau \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t)}), \\tag{32}\n",
    "$$\n",
    "\n",
    "with the momentum parameter $\\eta$, usually close to 1.0, i.e. $\\eta = 0.95$. In general, using past moments of the previously calculated iterations as a guide for the current gradient step to enhance the performance. <br /> \n",
    "ADAM uses a exponentially decaying average of the first and second memoments of the gradient to compute individual adaptive learning rates for each parameter independently. [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "The previous Section dealt with the Backpropagation and Gradient descent. However, to calculate a proper gradient, we utilize an activation function $f_l(x)$. In order to introduce non-linearities so that the model can also represent complex datasets, employing a non-linear activation function is essential. <br /> \n",
    "It is required that these functions are continous and differentiable in order for the backpropagation to work. [1] <br >\n",
    "Since the feedforward is just a nested function of the inputs times the activation functions, it can be written as: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{j=1}^M w_{1j}^L f_L \\left(\\sum_{i=1}^M w_{ji}^{L-1} f_{L-1} \\left(\\sum_{k=1}^M w_{ik}^{L-2} f_{L-2} \\left( \\ldots f_1(w_{m1}^1 x_1 + b_m^1) \\ldots \\right) +b_k^{L-2} \\right) + b_i^{L-1} \\right) + b_1^L. \\tag{33}\n",
    "$$\n",
    "\n",
    "The simplest activation function would be the identity transformation $f_I(x) = x$, which is often used for regression networks in the output layer.\n",
    "Common activation functions comprise the *sigmoid, ReLU, tanh, leaky ReLU* and $ELU$ function. \n",
    "The sigmoid function is commonly used as hidden layer activations, or as the output layer for binary classification tasks, since this function squeezes its input values to a range from $[0, \\ldots, 1]$ : <br /> \n",
    "\n",
    "$$\n",
    "f_{sigmoid}(x) = \\frac{1}{1 + e^{-x}}. \\tag{34}\n",
    "$$\n",
    "\n",
    "Next in line is the tangent hyperbolicus, that squeezes the values to a range from $[-1, \\ldots, 1]$: <br /> \n",
    "\n",
    "$$\n",
    "f_{tanh}(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}. \\tag{35}\n",
    "$$\n",
    "\n",
    "Simpler than both of the other activation functions and usually achieving better results are the Rectified Linear Units (ReLU). It consists of a piecewise linear function, when it reaches the origin of a coordinate system: <br /> \n",
    "\n",
    "$$\n",
    "f_{ReLU}(x) = max(0, x). \\tag{36}\n",
    "$$\n",
    "\n",
    "Many variants of ReLU exist, which are not 0 in the negative space, such as the leaky ReLU: <br /> \n",
    "\n",
    "$$\n",
    "f_{leaky ReLU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha x & if \\quad x < 0.\n",
    "\\end{cases}, \\tag{37}\n",
    "$$\n",
    "\n",
    "and the exponential linear unit (ELU): <br /> \n",
    "\n",
    "$$\n",
    "f_{ELU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha (e^x - 1) & if \\quad x < 0.\n",
    "\\end{cases}. \\tag{38}\n",
    "$$\n",
    "\n",
    "Fig. 3 visualizes each of these activation functions. <br /> \n",
    "![Activation Functions](plots/ActivationFunctions.png \"Activation Functions\")\n",
    "*Fig. 3 Different, common Activation functions used in Neural Networks. Retrieved from [Hackernoon](https://hackernoon.com/how-to-debug-neural-networks-manual-dc2a200f10f2) on the 8th October, 2019.*\n",
    "\n",
    "#### Exploding and vanishing gradients\n",
    "\n",
    "Looking at Fig. 3, one can see that the functions on the left hand-side (Sigmoid, tanh and ReLU) are only constant on a specific interval. For instance ReLU is 0 before the origin and then linearly increasing, tanh and sigmoid are only differentiable in an interval from approximately [-3, 3]. <br /> Thus, those activation functions could squeeze the input to 0 beyond the intervals, i.e. a fully saturated neuron with input $z_j >> 1$, or a dead neuron with input $z_j << -1$ will exhibit very small gradients or none at all. Resulting in wasted neurons, since they cannot learn anything. [9]\n",
    "To avoid this problem, it is important to initialize the weights and biases correctly. One method to initialize the weights and biases is the [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). Initialize the weights in such a way that the variance remains the same for x and y as we pass thorugh the layer. <br /> \n",
    "Picking the weights from a Gaussian distribution, $\\mathcal{N}(0, \\frac{1}{N})$, with zero mean and a variance of $1/N$, where $N$ specifies the number of input neurons: <br /> \n",
    "\n",
    "$$\n",
    "var(w_i) = \\frac{1}{N}. \\tag{39}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following work will focus on datasets retrieved from *[United Nations (UN)](http://data.un.org)*, *[World Health Organization (WHO)](https://www.who.int/data/gho/data/indicators)* as well as *[Food and Agriculture Organization of the United Nations (FAO)](http://www.fao.org/faostat/en/#data)*. Several files have been downloaded and merged together into one huge file of the datapoints. All the file cover mostly the same countries with different indicators, such as Cancer death rates, Vegetable consumption, GDP per capita, Life expectancy and many more. Most of these indicators are even tracked over several years, resulting in a huge dataset. <br /> \n",
    "However, since the dataset I want to work with comprises multiple single files, merging those is essential. Due to the fact that the following work is trusting this merged dataset, it is important to validate the data as well as prepare and process those in the proper manner. Since the datasets were created by different persons within that organization, sometimes the country names differ, i.e. in one list *Hong Kong* might be recorded like that but in another list it shows up as *China, Hong Kong SAR*, same with *Macao* or *Taiwan*. Likewise for *United States of America*, which can show up in too many different synonyms. Thus, this could end in a disaster, if the files are not processed before, since the data is not corresponding to the real, recorded values. <br /> \n",
    "There is an [extra notebook](https://github.com/lenlehm/Opportunity-Insights/blob/master/Create_Dataset.ipynb), which only takes care of the preprocessing and that everyone can follow my implementation strategies for merging the lists into one *Masterlist*. <br /> \n",
    "Furhtermore, there you will see that I defined a set of countries I am interested in, 196 countries in total, which counts to rename as a first step of preprocessing to ensure data consistency. <br /> \n",
    "In order to save computation and memory, the states which are not listed in the respective file will get deleted to reduce the size of the files. <br />\n",
    "Once the dataset is created, we have 197 countries with several indicators over the different years, resulting in a dataset matrix $X \\in \\mathbb{R}^{1918 \\times 197}$. <br />\n",
    "\n",
    "Due to the interest in the actual values and more interpretability in those I decided against standardization or normalization of the dataset. It would definitely help for the Classification and Regression Analysis for the respective Machine Learning algorithm, but since the focus of this work is on the analysis of the data, there will be no normalization or standardization. <br /> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are mostly two common techniques: **Min-Max Normalization**, as well as **Standardization**. However, when the minimum and maximum values are not known in the features, it is obviously not possible to apply the first technique, which reads as follows: <br /> \n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min{(x)}}{\\max{(x)} - \\min{(x)}}. \\tag{42}\n",
    "$$\n",
    "\n",
    "Equation (42) squeezes all the values in the respective column to a range from [0, ..., 1]. Additionally to the aforementioned problem, considerable outliers will affect this method greatly, where the outlier takes either the value $1$ when being the biggest value or $0$ when it is super small.  <br /> \n",
    "That is the reason why this work makes use of *Standardization*: <br />\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma_x}, \\tag{43}\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ is the mean/ average value of the feature vector and $\\sigma{x}$ is its standard deviation. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Code and Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lovely libraries that saved my ass here\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score, r2_score\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a plot directory as well as get the data directory\n",
    "if os.path.isdir(os.path.join(os.getcwd(), 'plots')): \n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')\n",
    "else :\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'plots'))\n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')\n",
    "    \n",
    "datapath = os.path.join(os.path.join(os.getcwd(), 'data'), 'Health')\n",
    "file_data = 'Merged UN Data 1980+.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html) seems to be superior to the other methods available in scikit Learn. Thus we are going to implement this guy along with KMeans, since it is so simple and intuitive. <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sweden</th>\n",
       "      <th>Norway</th>\n",
       "      <th>Finland</th>\n",
       "      <th>Iceland</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Netherlands</th>\n",
       "      <th>Belgium</th>\n",
       "      <th>Luxembourg</th>\n",
       "      <th>England</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>...</th>\n",
       "      <th>Cayman Islands</th>\n",
       "      <th>Botswana</th>\n",
       "      <th>Ethiopia</th>\n",
       "      <th>Chad</th>\n",
       "      <th>Uruguay</th>\n",
       "      <th>Maldives</th>\n",
       "      <th>Gibraltar</th>\n",
       "      <th>Paraguay</th>\n",
       "      <th>Niger</th>\n",
       "      <th>Nepal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agriculutral are per capita [h/person] in 1980</th>\n",
       "      <td>0.445674</td>\n",
       "      <td>0.229243</td>\n",
       "      <td>0.530655</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.233917</td>\n",
       "      <td>0.143395</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.056112</td>\n",
       "      <td>1.492550</td>\n",
       "      <td>10.669178</td>\n",
       "      <td>5.171811</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.074711</td>\n",
       "      <td>5.265684</td>\n",
       "      <td>0.283976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculutral are per capita [h/person] in 1981</th>\n",
       "      <td>0.441707</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.524203</td>\n",
       "      <td>8.225108</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>0.141970</td>\n",
       "      <td>0.143284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.148936</td>\n",
       "      <td>1.447753</td>\n",
       "      <td>10.442420</td>\n",
       "      <td>5.128790</td>\n",
       "      <td>0.044025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.087789</td>\n",
       "      <td>5.047508</td>\n",
       "      <td>0.278774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculutral are per capita [h/person] in 1982</th>\n",
       "      <td>0.439745</td>\n",
       "      <td>0.229309</td>\n",
       "      <td>0.515479</td>\n",
       "      <td>8.154506</td>\n",
       "      <td>0.233091</td>\n",
       "      <td>0.140830</td>\n",
       "      <td>0.150335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.281979</td>\n",
       "      <td>1.398935</td>\n",
       "      <td>10.209924</td>\n",
       "      <td>5.084970</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.098230</td>\n",
       "      <td>4.922179</td>\n",
       "      <td>0.272628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculutral are per capita [h/person] in 1983</th>\n",
       "      <td>0.436704</td>\n",
       "      <td>0.229369</td>\n",
       "      <td>0.504925</td>\n",
       "      <td>8.050847</td>\n",
       "      <td>0.232493</td>\n",
       "      <td>0.140421</td>\n",
       "      <td>0.150125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.471119</td>\n",
       "      <td>1.348270</td>\n",
       "      <td>9.968944</td>\n",
       "      <td>5.042718</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.132302</td>\n",
       "      <td>4.889625</td>\n",
       "      <td>0.266602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculutral are per capita [h/person] in 1984</th>\n",
       "      <td>0.433285</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.497172</td>\n",
       "      <td>7.949791</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>0.149753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.692845</td>\n",
       "      <td>1.293846</td>\n",
       "      <td>9.717457</td>\n",
       "      <td>4.998998</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.160967</td>\n",
       "      <td>4.720135</td>\n",
       "      <td>0.260730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Sweden    Norway   Finland  \\\n",
       "Indicator                                                                      \n",
       "Agriculutral are per capita [h/person] in 1980  0.445674  0.229243  0.530655   \n",
       "Agriculutral are per capita [h/person] in 1981  0.441707  0.228571  0.524203   \n",
       "Agriculutral are per capita [h/person] in 1982  0.439745  0.229309  0.515479   \n",
       "Agriculutral are per capita [h/person] in 1983  0.436704  0.229369  0.504925   \n",
       "Agriculutral are per capita [h/person] in 1984  0.433285  0.230769  0.497172   \n",
       "\n",
       "                                                 Iceland   Germany  \\\n",
       "Indicator                                                            \n",
       "Agriculutral are per capita [h/person] in 1980  8.333333  0.233917   \n",
       "Agriculutral are per capita [h/person] in 1981  8.225108  0.233568   \n",
       "Agriculutral are per capita [h/person] in 1982  8.154506  0.233091   \n",
       "Agriculutral are per capita [h/person] in 1983  8.050847  0.232493   \n",
       "Agriculutral are per capita [h/person] in 1984  7.949791  0.231954   \n",
       "\n",
       "                                                Netherlands   Belgium  \\\n",
       "Indicator                                                               \n",
       "Agriculutral are per capita [h/person] in 1980     0.143395  0.144330   \n",
       "Agriculutral are per capita [h/person] in 1981     0.141970  0.143284   \n",
       "Agriculutral are per capita [h/person] in 1982     0.140830  0.150335   \n",
       "Agriculutral are per capita [h/person] in 1983     0.140421  0.150125   \n",
       "Agriculutral are per capita [h/person] in 1984     0.140214  0.149753   \n",
       "\n",
       "                                                Luxembourg  England  Scotland  \\\n",
       "Indicator                                                                       \n",
       "Agriculutral are per capita [h/person] in 1980         0.0      0.0       0.0   \n",
       "Agriculutral are per capita [h/person] in 1981         0.0      0.0       0.0   \n",
       "Agriculutral are per capita [h/person] in 1982         0.0      0.0       0.0   \n",
       "Agriculutral are per capita [h/person] in 1983         0.0      0.0       0.0   \n",
       "Agriculutral are per capita [h/person] in 1984         0.0      0.0       0.0   \n",
       "\n",
       "                                                  ...     Cayman Islands  \\\n",
       "Indicator                                         ...                      \n",
       "Agriculutral are per capita [h/person] in 1980    ...                0.0   \n",
       "Agriculutral are per capita [h/person] in 1981    ...                0.0   \n",
       "Agriculutral are per capita [h/person] in 1982    ...                0.0   \n",
       "Agriculutral are per capita [h/person] in 1983    ...                0.0   \n",
       "Agriculutral are per capita [h/person] in 1984    ...                0.0   \n",
       "\n",
       "                                                 Botswana  Ethiopia  \\\n",
       "Indicator                                                             \n",
       "Agriculutral are per capita [h/person] in 1980  26.056112  1.492550   \n",
       "Agriculutral are per capita [h/person] in 1981  25.148936  1.447753   \n",
       "Agriculutral are per capita [h/person] in 1982  24.281979  1.398935   \n",
       "Agriculutral are per capita [h/person] in 1983  23.471119  1.348270   \n",
       "Agriculutral are per capita [h/person] in 1984  22.692845  1.293846   \n",
       "\n",
       "                                                     Chad   Uruguay  Maldives  \\\n",
       "Indicator                                                                       \n",
       "Agriculutral are per capita [h/person] in 1980  10.669178  5.171811  0.045455   \n",
       "Agriculutral are per capita [h/person] in 1981  10.442420  5.128790  0.044025   \n",
       "Agriculutral are per capita [h/person] in 1982  10.209924  5.084970  0.048485   \n",
       "Agriculutral are per capita [h/person] in 1983   9.968944  5.042718  0.046784   \n",
       "Agriculutral are per capita [h/person] in 1984   9.717457  4.998998  0.045198   \n",
       "\n",
       "                                                Gibraltar  Paraguay     Niger  \\\n",
       "Indicator                                                                       \n",
       "Agriculutral are per capita [h/person] in 1980        0.0  4.074711  5.265684   \n",
       "Agriculutral are per capita [h/person] in 1981        0.0  4.087789  5.047508   \n",
       "Agriculutral are per capita [h/person] in 1982        0.0  4.098230  4.922179   \n",
       "Agriculutral are per capita [h/person] in 1983        0.0  4.132302  4.889625   \n",
       "Agriculutral are per capita [h/person] in 1984        0.0  4.160967  4.720135   \n",
       "\n",
       "                                                   Nepal  \n",
       "Indicator                                                 \n",
       "Agriculutral are per capita [h/person] in 1980  0.283976  \n",
       "Agriculutral are per capita [h/person] in 1981  0.278774  \n",
       "Agriculutral are per capita [h/person] in 1982  0.272628  \n",
       "Agriculutral are per capita [h/person] in 1983  0.266602  \n",
       "Agriculutral are per capita [h/person] in 1984  0.260730  \n",
       "\n",
       "[5 rows x 197 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "filePath = os.path.join(datapath, file_data)\n",
    "df = pd.read_csv(filePath)\n",
    "\n",
    "# stdX = (data - data.mean()) / data.std()\n",
    "# normX = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "\n",
    "#df.head(5) # df.info()\n",
    "df = df.rename(columns={df.columns[0]: 'Indicator'})\n",
    "df.index = df[df.columns[0]]\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1515 entries, Agriculutral are per capita [h/person] in 1980 to Vegetables per capita [kg] in 2013\n",
      "Columns: 197 entries, Sweden to Nepal\n",
      "dtypes: float64(195), int64(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# to get a better feeling of what I put in the dataset, here is the index list: \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNO NULL VALUES\n",
      "Not quite true, I set some values manually to 0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values and target distribution\n",
    "if df.isnull().values.any():\n",
    "    # check which column\n",
    "    null_columns=data.columns[data.isnull().any()]\n",
    "    print(data[null_columns].isnull().sum())\n",
    "else: \n",
    "    print(\"\\tNO NULL VALUES\")\n",
    "    print(\"Not quite true, I set some values manually to 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d872885128>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimate the amount of clusters via elbow method\n",
    "\n",
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,40)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(df)\n",
    "    distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that the optimal cluster number is at 5 when iterating over 40 possible number of clusters. According to [datanovia](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/) the optimal cluster number is where the location of a bend (knee) occurs and this determines the appropriate number of clusters. The \"bend\" is considered where the value does not decrease as much as it did before, hence in our case 5 is the optimal number of clusters according to the Elbow method. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "       n_clusters=24, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "       random_state=None, tol=0.0001, verbose=0)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# cluster estimation via Bayesian Information Criterion (BIC) following the following paper\n",
    "# https://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf\n",
    "def compute_bic(kmeans,X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for a given clusters\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    "\n",
    "    X     :  multidimension np array of data points\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    BIC value\n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = [kmeans.cluster_centers_]\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    "\n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var = (1.0 / (N - m) / d) * sum([sum(cdist(X[np.where(labels == i)], [centers[0][i]], \n",
    "             'euclidean')**2) for i in range(m)])\n",
    "\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    "\n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    "\n",
    "    return(BIC)\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(np.array(df))\n",
    "\n",
    "ks = range(1,25)\n",
    "\n",
    "## run k times kmeans and save each result in the KMeans object\n",
    "kmeans = [KMeans(n_clusters=i, init=\"k-means++\").fit(X) for i in ks]\n",
    "\n",
    "print(kmeans[-1])\n",
    "\n",
    "## now run for each cluster the BIC computation\n",
    "BIC = [compute_bic(kmeans_i, X) for kmeans_i in kmeans]\n",
    "\n",
    "print(bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1918,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = 5\n",
    "# Cluster these babies\n",
    "clustering = SpectralClustering(n_clusters=clusters, assign_labels=\"discretize\", random_state=0).fit(X)\n",
    "print(clustering.labels_.shape)\n",
    "\n",
    "kMean = KMeans(n_clusters = clusters, random_state=0).fit(X) \n",
    "kMean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x239c2b40c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x239c2ae66d8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"Mean Houshold Income\"].hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the dataset is heavily unbalanced. <br /> \n",
    "Label 1 accounts for only 22% of the entire dataset. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name                                                Min Value      Max Value\n",
      "----------------------------------------------------  ---------------  -------------\n",
      "state                                                     1                72\n",
      "county                                                    1               840\n",
      "cz                                                      100             39400\n",
      "czname                                                22266.9          146195\n",
      "Mean Houshold Income                                      7.22966          46.0638\n",
      "Mean Commute Time                                         0.0488291         0.637071\n",
      "25+ year old academics fraction in 2000                   0.0372541         0.71074\n",
      "25+ year old academics in 2010                            0                 0.722387\n",
      "Foreign born Residents in 2010                        11640.3          129150\n",
      "Median Household Income 2016                           8701.05          61349.9\n",
      "Median Household Income 1990                              0                 0.645278\n",
      "Share of individuals below poverty in 2010                0                 0.56917\n",
      "Share of individuals below poverty line in 2010           0                 0.63009\n",
      "Share of individuals below poverty line in 1990           0                 0.856718\n",
      "Black Race Share in 2010                                  0                 0.996965\n",
      "Hispanic Race Share in 2010                               0                 0.427615\n",
      "Asian Race Share in 2010                                  0                 0.858614\n",
      "Black Race Share in 2000                                  0.0198624         0.996089\n",
      "White Race Share in 2000                                  0.000820345       0.975825\n",
      "Hispanic Race Share in 2000                               0                 0.454959\n",
      "Asian Race Share in 2000                                 -0.661327          6.57697\n",
      "Mean 3rd grade Math test scores                         172.16           2085.23\n",
      "Median gross rent for 2bd in 2015                         0                 0.805835\n",
      "Single Parent Share in 2010                               0                 0.588561\n",
      "Single Parent Share in 1990                               0.0247934         0.617654\n",
      "Single Parent Share in 2000                               0.0867728         0.990722\n",
      "16+ years old below 15min commute                         0.240604          0.836543\n",
      "Employment Rate in 2000                                   0               100\n",
      "The 2010 Census return rate                              -1.12622           0.9125\n",
      "Wage Growth Highschool grads                              0.00836819        0.997977\n",
      "Non-White Share in 2010                                   0.0384042     70583.6\n",
      "Population Density in 2010                                0             68416.7\n",
      "Population Density in 2000                               -0.0826727         0.121672\n",
      "Averaged Annual Job growth rate from 2004 until 2013      0.0153534     36663.2\n"
     ]
    }
   ],
   "source": [
    "print(tabulate( [ [name, mini, maxi] for name, mini, maxi in zip(data.columns, data.min(axis=0), data.max(axis=0))], headers=['Feature Name', 'Min Value', 'Max Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Fig. 3 shows the regularization parameter $\\lambda$ is not making any difference on neither the Scikit Learn Logistic Regression model, nor on my implementation. <br />\n",
    "They are constant for a wide range of $\\lambda$ and thus insensitive to this parameter change. Keep in mind that this could also be due to this given dataset. That the bias-variance tradeoff are not such a big problem for the default credit card dataset and thus the regularization parameter does not play a role. <br />\n",
    "We can conclude that Logistic Regression is not sensitive for $\\lambda$ for this dataset und thus makes it even more robust and stable. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curve (ROC)\n",
    "\n",
    "In order to compare the results from one algorithm to another, this work utilizes the ROC curve. A ROC curve is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: the *True Positive Rate (TPR)*, and the *False Positive Rate (FPR)*, which are defined as follows: <br />\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}, \\tag{44} \\quad \\textrm{where TP stands for True Positives and FN for False Negatives,}\n",
    "$$\n",
    "\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}, \\tag{45} \\quad \\textrm{where FP are False Positives and TN are True Negatives.}\n",
    "$$.\n",
    "\n",
    "A ROC curve plots TPR (y-axis) vs. FPR (x-axis) at different classification thresholds. Lowering the classification threshold, classifies more items as positive, thus increasing both False Positives and True Positives. <br /> \n",
    "Moreover, there is an addition to the ROC curve, calles *Area under ROC Curve (AUC)*, which represents the area underneath the 2D ROC curve (can be thought of as the integral of the ROC curve). <br /> \n",
    "AUC provides an aggregate measure of performance across all possible classification thresholds. <br />\n",
    "AUC is a commonly used metrics to evaluate a classification algorithm due to its scale - and classification threshold invariance. <br /> It measures how well predictions are ranked rather than their absolut values (scale-invariant) and the quality of the model's predictions irrespective of what classification threshold was chosen (classification-threshold-invariant). <br />\n",
    "The bigger AUC, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setup new Training data and Test data if wanted, but then keep in mind that we can't compare LR to NN\n",
    "new_data = False\n",
    "\n",
    "## Get new data potentially otherwise use the versions from the Log Reg\n",
    "if new_data:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2912)\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = train_test_split(stdX, stdY, test_size=0.2, random_state=2912)\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normX, normY, test_size=0.2, random_state=2912)\n",
    "else: \n",
    "    X_train, X_test, y_train, y_test = logistic.X_train, logistic.X_test, logistic.y_train, logistic.y_test\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = stdLogistic.X_train, stdLogistic.X_test, stdLogistic.y_train, stdLogistic.y_test\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = normLogistic.X_train, normLogistic.X_test, normLogistic.y_train, normLogistic.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c624ef0400>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.770\n"
     ]
    }
   ],
   "source": [
    "## Claculate and Plot the ROC curve\n",
    "probs = clf_norm.predict_proba(X_test_norm)\n",
    "probs = probs[:, 1]\n",
    "FPR, TPR, thresholds = roc_curve(y_test_norm, probs)\n",
    "plot_roc_curve(FPR, TPR, 5, \" ROC curve of MLP on normalized data\")\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, probs)))\n",
    "#print(\"My R2 Score on Neural Net: {}\".format(nn.R2)) # need to run Neural net 2 cells below first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c624f40da0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the lovely accuracies\n",
    "txt = \"Fig. 6 Effects of learning rate on Neural Network using SGD as optimizer.\"\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(learning_rates_to_test, accuracies, label=\"Test Accuracy\")\n",
    "plt.title(\"Accuracies for Neural Network for different learning rates\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"Î» parametrization values\")\n",
    "plt.xticks(learning_rates_to_test)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fig.text(.1, 0,txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Neural Network learning rate.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should not be the case here. <br /> \n",
    "Usually the learning rate is a crucial factor for convergence of the algorithm. In the Analysis part below there are some words about the learning rate parametrization and also the effects that should occur in those tests. <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the classification task\n",
    "\n",
    "One thing that straight catches the eye is how sensitive the neural network is to small parameter changes. All the parameters that have been changed in the above cells are called *hyperparameters*. <br /> \n",
    "Those are all the parameters that determine the succesand performance of the neural network like not only learning rate, regularization strength, activation functions, but also batch size, number of epochs and so on, which have not greatly been modified above due to time restrictions. <br />\n",
    "Furthermore, looking across all the different datasets one can see that the data preparation also takes a (big) stake for the model's performance, depending on which activation functions are being used (some could end up in vanishing or exploding gradients). <br />\n",
    "\n",
    "Unfortunately, Fig. 6, which shows the effects of a changing leraning rates $\\tau$ did not end up being the desired end result. Usually $\\tau$ is very critical for the success of learning and also convergence. <br />\n",
    "If the learning rate $\\tau$ is too small, it takes years until it converges (if enough epochs are set). If the learning rate $\\tau$ is too high, the algotihm can't converge at all, since it is only bouncing around and not resulting in a (global) optimum. [8] <br />\n",
    "Fig. 7 displays the effects of the regularization strength $\\lambda$.\n",
    "Whe $\\lambda$ is too high, the algorithm is penalzing too much ending up in a bad model and thus bad accuracy. Obviously, if $\\lambda$ is too low it is like not penalzing at all and thus also getting a bad model with a bad accuracy score, if there are inconsistencies in the dataset, where regularization would be needed. <br />\n",
    "\n",
    "Moving on to the last cell, where only the activation function was changed to a *tanh* activation instead of *ReLU*. <br /> \n",
    "The accuracy experienced a massive drop of almost 57%. Coming from almost 78% for a *ReLU* network with the same shape and size to 21%. Due to vanishing and exploding gradients, the network ends up not learning anything. <br />\n",
    "\n",
    "Those are just some few out of many parameters in a neural networks to tune. There are multiple more, like batch size, number of epochs, number of hidden layers, number of neurons, choice of optimizer and so on. <br /> \n",
    "Hence, one can conclude that neural networks are really sensitive to parameter changes along with a tedious search for optimal hyperparameters. <br /> \n",
    "\n",
    "When considering Logistic Regression in the beginning of this work, there were similiar results achieved without having to tune some critical parameters. <br /> \n",
    "Moreover, Logistic Regression's accuracy basically stayed the same even when changing its regularization parameter $\\lambda$, which makes it even more robust compared to the sensitive neural networks. <br />\n",
    "Due to its simplicity in implementation, faster convergence and no tedious hyperparameter search Logistic Regression should prefered over Neural Networks for this dataset. <br />\n",
    "\n",
    "However, there is to mention that when properly tuning the Neural Network hyperparameters with Bayesian Analysis [3], it is possible to further boost the Neural Network's accuracy, however this comes to the cost of a more complex implementation as well as computational power and time (= both of which I do not have). <br /> \n",
    "\n",
    "Lastly, when comparing the results to Yeh et al. [4] it is great to see that they achieved similiar results. Since they do not state, which kind of data preprocessing they utilized for their work and also did not clarify their train and test data split, some deviations are naturally, especially when considering that they most probably had different samples compared to mine in their test dataset even if they utilized a 20% split. <br />\n",
    "Nevertheless, the results are quite similiar with the same conclusion that Neural Networks are in terms of accuracy slightly better but not with respect to the implementation and searching complexity of these beasts. (Happy troubleshooting by the way - cost me like 2 days to find a small little bug) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "\n",
    "As mentioned in the theoretical part of Neural Networks, they can either be used for Regression or Classification (as done above). The Classification utilizes the cross entropy, equation (30), as a loss function. The neat benefit of the cross-entropy loss is that it is a convex function, meaning a local minimum is also automatically a global minimum. This is awesome for our minimization problem, since it is sure to find the global minimum of our function and thus the best result. <br /> \n",
    "In classification there is a very particular set of possible output values, the possibles classes. Hence, Mean-Squared Error (MSE), equation (29), is badly defined, as it does not have this kind of knowledge and in turn penalizes errors in an incompatible way. However, in Regression where the goal is to predict a continuous value, MSE gives a great indicator how far apart the predicted value to the true value is. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to Regression Analysis with the neural network. As mentioned perviously, the cost function changed from Cross Entropy Loss (for classification) to Mean Squared Error for Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work analysed several datasets from the WHO, UN and FAO covering a lot of the worlds countries data with respect to life expectancy, wealth, health and food. <br /> \n",
    "Due to time restrictions and to fulfill the bare minimum of machine learning algorithms used for the FYS STK4155 course, I did not try to tune any hyperparameters for the neural networks or XGBoost algorithms. <br />\n",
    "Taken into account that my Notebook has limited computing power - Intel Core i7, 7th generation, 16GB RAM and 512GB SSD - there was the bare minimum of hyperparameter search done for both Neural Nets and XGBoost. <br />\n",
    "The sweetness of XGBoost as well as Neural Networks is that both of these algorithms can be used for Regression and Classification on tabulated data with an extraordinary performance on both tasks. <br />\n",
    "Hyperparameters are essential parameters for XGBoost and Neural Networks, which need to be tuned to end up in an optimal solution. These hyperparameters include the learning rate $\\tau$, batch sizes, regularization parameter $\\lambda$, amount of hidden layers, amount of neurons within each layer, activation functions, epochs and optimizer (SGD or Adam). <br /> \n",
    "Hence, there is even more potential for Neural Networks with the given dataset, when performing a hyperparameter search by using either random search or Bayesian Analysis. [2] <br />\n",
    "\n",
    "However, since I was mostly focussing on the Analysis part of the data, which revealed some interesting insights about\n",
    "**THIS IS BY FAR NOT DONE **\n",
    "### GIVE ME MORE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Conclusions\n",
    "\n",
    "This work shows that it is possible to derive some information about the correlation about life expectancy and nutrition even on a country based filter. <br /> \n",
    "\n",
    "**There are some countries building clusters and are thus similiar with respect to the dataset we have at hand.**\n",
    "\n",
    "** WRITE SOMETHING ABOUT CLASSIFICATION AND REGRESSION **\n",
    "\n",
    "Keep in mind that these results have to be seen critically, since there is a lot of bias in the data due to tourism and other external factors that might affect the recordings of some values. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Further Work \n",
    "\n",
    "This work has shown some interesting facts about life expectancy and still has a lot more potential. However, to leverage the entire, irrepressible power of the UN, FAO and WHO datasets, it would be amazing to even add more indicator variables to the dataset. There are so many more interesting recordings on their websites such as *machinery usage*, *producer prices* or *air pollution*. Due to the lack of time, I was not able to incorporate those into my dataset. <br /> \n",
    "Furthermore, it would be a great to analyze countries with a similiar human development index (HDI) and further derive conclusions about the differences among those \"similiar\" countries. Using multiple datasets, splitted according to the HDI, would not only speed up the findings due to reduced computational power, but is also more transparent and eases the analysis task. <br />\n",
    "Comparing similiar developed countries can then be used to derive insights why states are performing better than others and in turn help those \"weaker\" countries to become stronger. <br /> \n",
    "There is still a lot of potential hidden in the data, I restricted myself to the nutrition/ life expectancy topics, but there are so many different questions which can be answered with the help of these amazing datasets of national organizations. <br /> \n",
    "Making full use of these datasets, it is possible to further reduce poverty, increase the overall human development index as well as the happiness of each individual by sharing the insights with the policy makers and foundations. <br />\n",
    "Thus, it is important to spread the word of the existence of such great datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Murphy, K. P. et al. (2007). *Machine Learning: A probabilistic Perspective*. Camebridge: The MIT Press. <br />\n",
    "[2] Bishop, C.M. et al. (2011). *Pattern Recognition and Machine Learning*. Cambridge: Springer. <br />\n",
    "[3] Chen, T. et al. (2016). *XGBoost: A Scalable Tree Boosting System*. Paper retrieved on 25th November 2019. https://arxiv.org/pdf/1603.02754.pdf <br />\n",
    "[4] Andersen, K. et al. (2017). *What the Health*. Movie distributed by A.U.M. Films & Media, last accessed on 3rd December 2019 https://www.whatthehealthfilm.com/facts <br />\n",
    "[5] Cameron, J. et al. (2018). *The Game Changers*. Movie, last accessed on 3rd December 2019 https://gamechangersmovie.com <br />\n",
    "[6]  U.S. Department of Health and Human Services and U.S. Department of Agriculture. *2015â€“2020 Dietary\n",
    "Guidelines for Americans.* 8th Edition. December 2015. Available at http://health.gov/dietaryguidelines/2015/guidelines/ , last accessed on 3rd December 2019<br />\n",
    "[7] Roser, M. et al. (2019). *Our World in Data*. Website: https://ourworldindata.org, last accessed 15th December 2019. <br />\n",
    "[8] Hastie, T. et al. (2009). *The Elements of Statistical Learning*. Camebridge: Springer <br />\n",
    "[9] Knoll, A. et al. (2019). *Cognitive Systems*. Lecture taught at Technical University Munich in the summer term 2019.\n",
    "[10] Diederik, P. et al. (2014). *Adam: A method for stochastic optimization*. Paper, ArXiv https://arxiv.org/abs/1412.6980, last accessed 14th December 2019. <br />\n",
    "[11] Tibshirani, R. et al. (2000). *Estimating the number of clusters in a data set via the gap statistics*. Paper: http://web.stanford.edu/~hastie/Papers/gap.pdf, last accessed 14th December 2019. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "Some more plots for the terrain data $\\lambda$ parametrization. <br /> \n",
    "All of the plots below are showing the Mean Squared Error (MSE) of Ridge or Lasso, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code, which has been used in project 1 only without noisy targets. <br />\n",
    "This code was used for the Terrain data Regression Analysis, which was shown towards the end of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "def plot_roc_curve(FPR, TPR, plot_number, text=\"ROC curve\"):  \n",
    "    txt = \"Fig. \" + str(plot_number) + text\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    plt.plot(FPR, TPR, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    fig.text(.1, 0,txt)\n",
    "    plt.savefig(os.path.join(plot_dir, 'ROC ' + str(plot_number) + '.png'), transparent=True, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
